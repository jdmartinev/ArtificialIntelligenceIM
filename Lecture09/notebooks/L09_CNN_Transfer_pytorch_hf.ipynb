{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1zpf4epZIziJJAlrZHcEP6cLEF-W8llK8",
      "authorship_tag": "ABX9TyNgTGKduXRlUSxqGMqaFBT4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdmartinev/ArtificialIntelligenceIM/blob/main/Lecture09/notebooks/L09_CNN_Transfer_pytorch_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transferencia de Aprendizaje en el Conjunto de Datos Caltech101\n",
        "\n",
        "En este cuaderno, consideraremos un conjunto de datos más complejo que MNIST o CIFAR10. Las imágenes en Caltech101 son imágenes RGB (3 canales) con tamaño variable. Hay 101 clases diferentes. Intentaremos una práctica muy común en visión por computadora en la actualidad: transferencia de aprendizaje desde un modelo preentrenado en ImageNet.\n",
        "\n",
        "## Plan de Trabajo:\n",
        "- Modificar la red del ejercicio anterior (CIFAR-10) para trabajar con imágenes de 224x224.\n",
        "- Entrenar el modelo por un tiempo en Caltech101 y ver qué tan lejos podemos llegar.\n",
        "- Tomar un ResNet34 que fue preentrenado en ImageNet-1k y ajustarlo a Caltech101.\n",
        "  - Considerar tanto entrenar solo la cabeza (el clasificador lineal al final de la red) como toda la red.\n",
        "  - Deberíamos poder alcanzar un mejor rendimiento que nuestra red original en menos pasos de entrenamiento.\n",
        "- Opcional: experimentar con otros modelos preentrenados de `timm` (ver información [aquí](https://github.com/rwightman/pytorch-image-models))."
      ],
      "metadata": {
        "id": "qj9xP5j599Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR8nmQB_XaBJ",
        "outputId": "3b9d9d11-b4d0-40d3-9b1b-7ddd70596c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminares"
      ],
      "metadata": {
        "id": "h2o5PEgM-CAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu6ync4s9ToL"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from typing import List, Optional, Callable, Iterator\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "def show_image(img, title=None):\n",
        "    img = img.detach().cpu()\n",
        "    img = img.permute((1, 2, 0)).numpy()\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean   # unnormalize\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.gca().tick_params(axis=\"both\", which=\"both\", bottom=False, left=False, labelbottom=False, labelleft=False)\n",
        "    if title is not None:\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de la base de datos"
      ],
      "metadata": {
        "id": "QsGxVBpl-LtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImageNet normalization\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "default_imagenet_normalization = transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "\n",
        "# --- Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.25, 1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    default_imagenet_normalization,\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    default_imagenet_normalization,\n",
        "])\n",
        "\n",
        "# Step 1: Load dataset\n",
        "hf_dataset = load_dataset(\"flwrlabs/caltech101\", split=\"train\")  # all in one split\n",
        "\n",
        "# Step 2: Count class occurrences\n",
        "label_counts = Counter(hf_dataset['label'])\n",
        "\n",
        "# Step 3: Keep only classes with ≤100 samples\n",
        "keep_classes = sorted([label for label, count in label_counts.items() if count <= 100])\n",
        "\n",
        "# Step 4: Subsample max 100 samples per class\n",
        "indices = list(range(len(hf_dataset)))\n",
        "random.shuffle(indices)\n",
        "\n",
        "new_indices = []\n",
        "class_counts = {i: 0 for i in keep_classes}\n",
        "\n",
        "for i in indices:\n",
        "    y = hf_dataset[i]['label']\n",
        "    if y in keep_classes and class_counts[y] < 100:\n",
        "        new_indices.append(i)\n",
        "        class_counts[y] += 1\n",
        "\n",
        "# Step 5: Translate labels\n",
        "def translate_label(y, keep_classes):\n",
        "    try:\n",
        "        return keep_classes.index(y)\n",
        "    except ValueError:\n",
        "        return -1\n",
        "\n",
        "labels = [translate_label(hf_dataset[i]['label'], keep_classes) for i in new_indices]\n",
        "assert max(labels) == len(keep_classes) - 1\n",
        "\n",
        "labels = np.array(labels)\n",
        "# Step 6: Train-test split\n",
        "test_size = 640\n",
        "val_size = 640\n",
        "train_size = len(new_indices) - test_size - val_size\n",
        "\n",
        "# First, split into train+val and test\n",
        "train_val_idx, test_idx = train_test_split(\n",
        "    new_indices,\n",
        "    test_size=test_size,\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Map from full dataset index to position in new_indices\n",
        "idx_map = {idx: i for i, idx in enumerate(new_indices)}\n",
        "\n",
        "# Get label indices for train_val\n",
        "train_val_labels = np.array([labels[idx_map[i]] for i in train_val_idx])\n",
        "\n",
        "# Then, split train+val into train and val\n",
        "train_idx, val_idx = train_test_split(\n",
        "    train_val_idx,\n",
        "    test_size=val_size,\n",
        "    stratify=train_val_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- Torch Dataset wrapper\n",
        "class HFCaltech101(Dataset):\n",
        "    def __init__(self, hf_dataset, indices, transform, keep_classes):\n",
        "        self.hf_dataset = hf_dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "        self.keep_classes = keep_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.hf_dataset[self.indices[idx]]\n",
        "        image = item['image'].convert('RGB')\n",
        "        label = translate_label(item['label'], self.keep_classes)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# --- Datasets and loaders\n",
        "train_dataset = HFCaltech101(hf_dataset, train_idx, train_transform, keep_classes)\n",
        "val_dataset = HFCaltech101(hf_dataset, val_idx, test_transform, keep_classes)\n",
        "test_dataset = HFCaltech101(hf_dataset, test_idx, test_transform, keep_classes)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w8oWEh5ZMKM",
        "outputId": "7f51e8a1-3789-4e16-f867-16047bc9f539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Un Vistazo Más Cercano al Conjunto de Datos\n",
        "\n",
        "Primero, trazamos el tamaño de cada clase y observamos que la distribución de clases no es uniforme.\n",
        "\n",
        "Luego, mostramos ejemplos aleatorios del conjunto de datos, anotados con la etiqueta de la clase y el índice.\n",
        "Ten en cuenta que las imágenes de entrenamiento incluyen aumentos estándar que se utilizan típicamente en modelos de visión (definidos anteriormente).\n"
      ],
      "metadata": {
        "id": "0OAWxaej-e8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_label_names = hf_dataset.features['label'].names\n",
        "categories = [full_label_names[i] for i in keep_classes]\n",
        "\n",
        "label_idxs, counts = np.unique(labels, return_counts=True)\n",
        "plt.figure(figsize=(20, 4.2))\n",
        "sns.barplot(\n",
        "    x=[categories[label] for label in label_idxs],\n",
        "    y=counts,\n",
        "    color=sns.color_palette('muted')[0]\n",
        ")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Number of examples\")\n",
        "plt.title(\"Class distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def show_dataset_examples(dataloader):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    with sns.axes_style(\"white\"):\n",
        "        fig, axes = plt.subplots(4, 8, figsize=(16, 9.5))\n",
        "    axes = [ax for axes_ in axes for ax in axes_]   # flatten\n",
        "\n",
        "    for j, (img, label) in enumerate(zip(images[:32], labels[:32])):\n",
        "        plt.sca(axes[j])\n",
        "        show_image(img, title=f\"{categories[label.item()]} ({label.item()})\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n\\nTrain images (including augmentations):\")\n",
        "show_dataset_examples(train_loader)\n",
        "print(\"\\n\\nTrain images (including augmentations):\")\n",
        "show_dataset_examples(val_loader)\n",
        "print(\"\\n\\nTest images:\")\n",
        "show_dataset_examples(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RvSNIeX4-irO",
        "outputId": "5e7c8c42-414d-4056-e3b3-ce0790170baa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir una Red Neuronal\n",
        "\n",
        "**Asignación 1:** Adapta la CNN del laboratorio anterior (CIFAR-10) para manejar imágenes de 224x224. Recomendamos reducir significativamente el tamaño de los tensores antes de aplanarlos, añadiendo capas convolucionales con `stride > 1` o capas de [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) (pero también puedes considerar [AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html)).\n"
      ],
      "metadata": {
        "id": "nyLUh4lV-x9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = ...   # Your code here!\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "model = Model(num_classes=len(np.unique(labels)))\n",
        "device = torch.device('cuda')  # use cuda or cpu\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_GKJ66F-7YJ",
        "outputId": "dfe579ba-9bd0-4239-f0d4-9af2bc95246c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir la Función de Pérdida y el Optimizador\n",
        "\n",
        "**Asignación 2:** Implementa el criterio y el optimizador, como en el cuaderno anterior."
      ],
      "metadata": {
        "id": "1FsfDC6b_Rnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = None  # Your code here!\n",
        "optimizer = None  # Your code here!"
      ],
      "metadata": {
        "id": "LPTspIkq_XCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar la red"
      ],
      "metadata": {
        "id": "6TuB4ayd_YQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the forward pass with dummy data\n",
        "out = model(torch.randn(2, 3, 224, 224).to(device))\n",
        "print(\"Output shape:\", out.size())"
      ],
      "metadata": {
        "id": "DwGXHNpA_a2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(targets, predictions):\n",
        "    return (predictions == targets).float().mean().item()\n",
        "\n",
        "num_epochs = 10\n",
        "validation_every_steps = 50\n",
        "\n",
        "step = 0\n",
        "model.train()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_accuracies_batches = []\n",
        "    train_loss_accum = 0\n",
        "    train_loss_steps = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # --- Forward pass\n",
        "        output = model(inputs)\n",
        "\n",
        "        # --- Loss\n",
        "        loss = loss_fn(output, targets)\n",
        "        train_losses.append(loss.item())\n",
        "        train_loss_accum += loss.item()\n",
        "        train_loss_steps += 1\n",
        "\n",
        "        # --- Backprop and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- Accuracy\n",
        "        predictions = output.argmax(dim=1)\n",
        "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
        "\n",
        "        step += 1\n",
        "\n",
        "        # --- Validation\n",
        "        if step % validation_every_steps == 0:\n",
        "            avg_train_loss = train_loss_accum / train_loss_steps\n",
        "            avg_train_acc = np.mean(train_accuracies_batches)\n",
        "            train_accuracies.append(avg_train_acc)\n",
        "\n",
        "            train_loss_accum = 0\n",
        "            train_loss_steps = 0\n",
        "            train_accuracies_batches = []\n",
        "\n",
        "            model.eval()\n",
        "            val_accuracies_batches = []\n",
        "            val_loss_accum = 0\n",
        "            num_val_samples = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = model(inputs)\n",
        "\n",
        "                    val_loss = loss_fn(output, targets)\n",
        "                    val_loss_accum += val_loss.item() * len(inputs)\n",
        "                    num_val_samples += len(inputs)\n",
        "\n",
        "                    predictions = output.argmax(dim=1)\n",
        "                    val_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
        "\n",
        "            val_accuracy = np.sum(val_accuracies_batches) / num_val_samples\n",
        "            avg_val_loss = val_loss_accum / num_val_samples\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            print(f\"Step {step:<5}   \"\n",
        "                  f\"train loss: {avg_train_loss:.4f}   train acc: {avg_train_acc:.4f}   \"\n",
        "                  f\"val loss: {avg_val_loss:.4f}   val acc: {val_accuracy:.4f}\")\n",
        "\n",
        "            model.train()\n",
        "\n",
        "print(\"Finished training.\")"
      ],
      "metadata": {
        "id": "WGR_GWpo_fF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluar la red en el conjunto de test"
      ],
      "metadata": {
        "id": "OJQ_DcWO_h2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_accuracies = []\n",
        "    num_samples = 0\n",
        "\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        output = model(inputs)\n",
        "        predictions = output.argmax(dim=1)\n",
        "\n",
        "        # Accumulate correct predictions × batch size\n",
        "        batch_acc = accuracy(targets, predictions) * len(inputs)\n",
        "        test_accuracies.append(batch_acc)\n",
        "        num_samples += len(inputs)\n",
        "\n",
        "    test_accuracy = np.sum(test_accuracies) / num_samples\n",
        "    print(f\"Test accuracy: {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "DaEorgsW_mEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando un Modelo Preentrenado\n",
        "\n",
        "Aquí cargaremos un ResNet34 que fue preentrenado en ImageNet. Luego, descartaremos el clasificador lineal al final de la red (la \"cabeza\" de la red) y lo reemplazaremos con uno nuevo que produzca el número deseado de logits para la clasificación. Para obtener una idea general de la estructura del modelo, lo imprimimos a continuación.\n",
        "\n",
        "El argumento `finetune_entire_model` en `initialize_model()` controla si se ajusta toda la red preentrenada. Cuando esto es `False`, solo se entrena la cabeza lineal y el resto del modelo se mantiene fijo. La idea es que las características extraídas por el modelo de ImageNet, hasta la capa de clasificación final, también son muy informativas en otros conjuntos de datos (ver, por ejemplo, [este artículo](https://arxiv.org/abs/1910.04867) sobre la transferibilidad de representaciones profundas en modelos de visión grandes).\n",
        "\n",
        "Comenzaremos aquí entrenando solo la cabeza lineal. Puedes experimentar con diferentes modelos y variaciones.\n",
        "\n",
        "A continuación, definimos el modelo y descartamos el que acabamos de entrenar. Después de eso, puedes volver a la sección \"Definir la Función de Pérdida y el Optimizador\" y volver a ejecutar el cuaderno desde allí, para entrenar y evaluar el nuevo modelo.\n"
      ],
      "metadata": {
        "id": "01ojnP3G_pzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name: str, *, num_classes: int, finetune_entire_model: bool = False):\n",
        "    \"\"\"Initializes a torchvision model with a new classifier and optionally freezes the backbone.\n",
        "\n",
        "    Returns:\n",
        "        model: the PyTorch model\n",
        "        info: a dict with total and trainable parameter counts\n",
        "    \"\"\"\n",
        "\n",
        "    print(\n",
        "        f\"Loading torchvision model '{model_name}', with \"\n",
        "        f\"finetune_entire_model={finetune_entire_model}, changing the \"\n",
        "        f\"last layer to output {num_classes} logits.\"\n",
        "    )\n",
        "\n",
        "    # Supported torchvision models (you can extend this list)\n",
        "    available_models = {\n",
        "        \"resnet18\": models.resnet18,\n",
        "        \"resnet34\": models.resnet34,\n",
        "        \"resnet50\": models.resnet50,\n",
        "        \"mobilenet_v2\": models.mobilenet_v2,\n",
        "        \"efficientnet_b0\": models.efficientnet_b0,\n",
        "        \"densenet121\": models.densenet121,\n",
        "    }\n",
        "\n",
        "    if model_name not in available_models:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported.\")\n",
        "\n",
        "    # Load the model with pretrained weights\n",
        "    model = available_models[model_name](weights=\"DEFAULT\")\n",
        "\n",
        "    # Replace final classifier\n",
        "    if \"resnet\" in model_name or \"resnext\" in model_name:\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(in_features, num_classes)\n",
        "        classifier_layer = model.fc\n",
        "    elif \"mobilenet\" in model_name:\n",
        "        in_features = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "        classifier_layer = model.classifier[1]\n",
        "    elif \"efficientnet\" in model_name:\n",
        "        in_features = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "        classifier_layer = model.classifier[1]\n",
        "    elif \"densenet\" in model_name:\n",
        "        in_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(in_features, num_classes)\n",
        "        classifier_layer = model.classifier\n",
        "    else:\n",
        "        raise ValueError(f\"Model structure not recognized for '{model_name}'.\")\n",
        "\n",
        "    # Optionally freeze all layers except the final classifier\n",
        "    if not finetune_entire_model:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        classifier_layer.weight.requires_grad_()\n",
        "        classifier_layer.bias.requires_grad_()\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    return model, {\n",
        "        \"num_model_parameters\": total_params,\n",
        "        \"num_trainable_model_parameters\": trainable_params,\n",
        "    }\n",
        "\n",
        "model, data = initialize_model('resnet34', num_classes=len(np.unique(labels)), finetune_entire_model=False)\n",
        "\n",
        "print(model)\n",
        "print(\"Number of model parameters:\", data[\"num_model_parameters\"])\n",
        "print(\"Number of trainable parameters:\", data[\"num_trainable_model_parameters\"])\n",
        "\n",
        "device = torch.device('cuda')  # use cuda or cpu\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "OizUhxEc_-UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Asignación 3:**\n",
        "\n",
        "1. Entrena el clasificador lineal sobre la red preentrenada y observa qué tan rápido puedes obtener buenos resultados, en comparación con entrenar una red más pequeña desde cero como hicimos anteriormente.\n",
        "\n",
        "2. Vuelve atrás y cambia el argumento para ajustar toda la red (`finetune_entire_model`), tal vez ajusta la tasa de aprendizaje, y observa si puedes obtener un mejor rendimiento que antes y si encuentras algún problema.\n",
        "\n",
        "3. Opcional: experimenta con `timm`: prueba modelos más pequeños o más grandes, incluyendo modelos de última generación, por ejemplo, basados en transformadores de visión (ViT) o MLP-Mixers.\n",
        "\n",
        "4. Describe brevemente lo que hiciste y cualquier experimento que realizaste, así como los resultados que obtuviste.\n",
        "¿Algo te sorprendió durante el ejercicio?\n",
        "\n",
        "5. Escribe las lecciones o ideas clave que obtuviste durante este ejercicio.\n"
      ],
      "metadata": {
        "id": "1P4fvccOANv6"
      }
    }
  ]
}