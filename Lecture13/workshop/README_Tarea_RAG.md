
# 游닄 Tarea: Construcci칩n de un Sistema RAG

## Descripci칩n

Para esta tarea, deber치s construir un sistema simple de **pregunta-respuesta** utilizando la t칠cnica de **Retrieval-Augmented Generation (RAG)**.

Una vez termines el desarrollo, deber치s compartir los resultados generados a 50 preguntas predefinidas.

Por favor sigue atentamente las instrucciones de configuraci칩n y desarrollo que encontrar치s a continuaci칩n.

---

## 1. Requisitos Previos

Para completar esta tarea necesitar치s:

- Python 3.10 o superior con las librer칤as de los ejemplos en clase instaladas
- Una clave de API de Groq

---

## 2. Configuraci칩n

- Crea el archivo `api_keys.env.
- Configura tu clave `GROQ_API_KEY` en el archivo `.env`:
  ```bash
  GROQ_API_KEY=...
  ```

---

## 3. Implementaci칩n de un Sistema de Pregunta-Respuesta usando RAG

En esta tarea implementar치s un sistema RAG que responda preguntas basadas en la transcripci칩n de la famosa charla de Andrey Karpathy: ["[1hr Talk] Intro to Large Language Models"](https://www.youtube.com/watch?v=zjkBMFhNj_g).

La transcripci칩n se encuentra en [intro-to-llms-karpathy.txt](docs/intro-to-llms-karpathy.txt).

Tu pipeline RAG debe incluir los siguientes pasos:

- **Ingesta de datos**:
  - Divide el documento en fragmentos peque침os.
  - Genera embeddings para cada fragmento.
  - Almacena los embeddings en una base de datos vectorial.
- **RAG**:
  - Genera el embedding de una pregunta.
  - Recupera los fragmentos relevantes de la base de datos.
  - Env칤a la pregunta y los fragmentos al LLM para generar una respuesta.

Un ejemplo b치sico usando LangChain en Python podr칤a verse as칤:

```python
hfEmbeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
loader = TextLoader("intro-to-llms-karpathy.txt")
index = VectorstoreIndexCreator(embedding=hfEmbeddings).from_loaders([loader])
llm = init_chat_model("llama3-8b-8192", model_provider="groq")
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=index.vectorstore.as_retriever(),
    return_source_documents=True,
)

question = "쯈u칠 es retrieval augmented generation y c칩mo mejora las capacidades de los modelos de lenguaje?"
result = qa_chain({"query": question})
```

**Recursos recomendados**:

- Python:
  - [Tutorial de RAG con LangChain](https://python.langchain.com/v0.2/docs/tutorials/rag/)
  - [Q&A RAG con LLamaIndex](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/q_and_a/#semantic-search)
---

## 4. Generaci칩n de Respuestas a la Lista de Preguntas

Se ha preparado un conjunto de **50 preguntas** que tu pipeline RAG deber치 responder.

Puedes encontrar estas preguntas en el archivo [questions.json](docs/questions.json).

Debes escribir un script que:

- Recupere los contextos relevantes para cada pregunta.
- Genere una respuesta usando tu pipeline RAG.
- Guarde las preguntas, los contextos recuperados y las respuestas en un archivo JSON con el siguiente formato:

```json
[
  {
    "question": "쯈u칠 es RAG?",
    "answer": "RAG es...",
    "contexts": [
      "Fragmento del documento 1...",
      "Fragmento del documento 2..."
    ]
  }
]
```

Un ejemplo simple en Python podr칤a ser:

```python
import json

json_results = []
for question in test_questions:
    response = qa_chain.invoke({"query": question})
    
    json_results.append({
        "question": question,
        "answer": response["result"],
        "contexts": [context.page_content for context in response["source_documents"]]
    })

with open('./my_rag_output.json', mode='w', encoding='utf-8') as f:
    f.write(json.dumps(json_results, indent=4))
```

---

## Notas Finales

- No debes alterar las preguntas.
- Aseg칰rate de enviar:
  - El c칩digo de tu pipeline RAG.
  - El archivo JSON con las preguntas, contextos y respuestas.



  

**Atenci칩n**: El conjunto de datos utilizado proviene de la transcripci칩n del video "[1hr Talk] Intro to Large Language Models" de Andrej Karpathy, bajo licencia Creative Commons Attribution (CC-BY).
---
