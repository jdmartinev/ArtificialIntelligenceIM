{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.5.10)\n",
            "Requirement already satisfied: requests in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.45.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.24.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: certifi in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jdmartinev\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# instalar dependencias\n",
        "%pip install transformers\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWBFHbzS20OQ",
        "outputId": "6688528e-4d45-4279-9066-12dc0ed9f90f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jdmartinev\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': '5 stars', 'score': 0.9009671807289124}]\n"
          ]
        }
      ],
      "source": [
        "# clasificación de texto\n",
        "# modelo preentrenado de OpenAI para clasificar reseñas de clientes en positivas o negativas:\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "result = classifier(\"Este producto es increíble, me encantó.\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "c:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jdmartinev\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9965198040008545}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# analisis de sentimientos\n",
        "\n",
        "from transformers import pipeline\n",
        "analyzer = pipeline(\"sentiment-analysis\")\n",
        "analyzer(\"No me gustó el servicio, fue una experiencia terrible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jdmartinev\\.cache\\huggingface\\hub\\models--deepset--bert-base-cased-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "modelo de lenguaje\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo 1 con BERT-QA\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/bert-base-cased-squad2\")\n",
        "\n",
        "context = \"El modelo de lenguaje BERT fue desarrollado por Google AI en 2018.\"\n",
        "question = \"¿Quién desarrolló BERT?\"\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "print(result['answer'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.03815583884716034,\n",
              " 'start': 3,\n",
              " 'end': 21,\n",
              " 'answer': 'modelo de lenguaje'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 6\u001b[0m client \u001b[39m=\u001b[39m OpenAI(\n\u001b[0;32m      7\u001b[0m     api_key\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49menviron\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mOPENAI_API_KEY\u001b[39;49m\u001b[39m\"\u001b[39;49m),  \u001b[39m# This is the default and can be omitted\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m#api_key=\"your-openai-api-key\"\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m     12\u001b[0m     messages\u001b[39m=\u001b[39m[\n\u001b[0;32m     13\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-4o\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n",
            "File \u001b[1;32mc:\\Users\\jdmartinev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "# ejemplo 2 - Uso de GPT-4 para Q&A Generativo\n",
        "# actualización: https://github.com/openai/openai-python\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"¿Quién descubrió la gravedad?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo 3: Q&A Mejorado con Recuperación de Información (RAG)\n",
        "# se verá más adelante!!!!\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "# Cargar la base de datos de documentos en ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# Crear un retriever para buscar información en la base de datos\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Crear la Conversational Retrieval Chain con GPT-4\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Hacer una pregunta con recuperación de documentos\n",
        "query = \"¿Qué es LangChain?\"\n",
        "response = qa_chain({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarization - resumen - Ejemplo con T5\n",
        "\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"Los modelos de lenguaje han cambiado la forma en que interactuamos con la tecnología...\"\n",
        "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generación de texto - Ejemplo con GPT-4\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Escribe un poema sobre la inteligencia artificial.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o\",\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chatbots y Asistentes Virtuales - Ejemplo con LangChain y GPT-4\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chatbot = ChatOpenAI(model_name=\"gpt-4\")\n",
        "response = chatbot.predict(\"¿Cuáles son los beneficios de la IA?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo 3: Q&A Mejorado con Recuperación de Información (RAG)\n",
        "# Uso de LangChain con ChromaDB\n",
        "#\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
        "    #api_key=\"your-openai-api-key\"\n",
        ")\n",
        "# Cargar la base de datos de documentos en ChromaDB\n",
        "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# Crear un retriever para buscar información en la base de datos\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Crear la Conversational Retrieval Chain con GPT-4\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(model_name=\"gpt-4\"),\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Hacer una pregunta con recuperación de documentos\n",
        "query = \"¿Qué es LangChain?\"\n",
        "response = qa_chain({\"question\": query})\n",
        "print(response[\"answer\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf798c3b36147777121152bbdc6069eee542c252cc637c99cf44c63253703aa6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
