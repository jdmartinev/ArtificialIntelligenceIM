{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7pygIoPwd5j8pEYEoGdO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdmartinev/ArtificialIntelligenceIM/blob/main/Lecture02/notebooks/L02_hyperparameter_tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "vMgg9y36Yfb3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-2BWolNYStm",
        "outputId": "8fee82c7-3f12-463e-9743-14df4b7560ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos para entrenar y validar: (404, 13)\n",
            "Datos para prueba final: (102, 13)\n"
          ]
        }
      ],
      "source": [
        "# 1. Cargar la base de datos de Boston Housing\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=False, parser='liac-arff')\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# 2. Separar un conjunto de prueba final (20%) que no tocaremos\n",
        "# El resto (80%) se usará para entrenamiento y cross-validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Escalar las características (muy importante para RBFs)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Datos para entrenar y validar: {X_train.shape}\")\n",
        "print(f\"Datos para prueba final: {X_test.shape}\")\n",
        "\n",
        "# Definir la función del kernel RBF que usaremos\n",
        "def rbf_kernel(X1, X2, gamma_param):\n",
        "    # np.newaxis ayuda a que las operaciones de broadcasting funcionen correctamente\n",
        "    dist_sq = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
        "    return np.exp(-dist_sq / lambda_param)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Versión 1: K-Fold Manual ---\n",
        "\n",
        "print(\"\\n--- Iniciando K-Fold Manual ---\")\n",
        "gamma_candidates = [10, 50, 100]\n",
        "n_centers = 50\n",
        "ridge_alpha = 0.1\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "mean_errors = {}\n",
        "\n",
        "for gamma_val in gamma_candidates:\n",
        "    fold_errors = []\n",
        "    for train_indices, val_indices in kf.split(X_train):\n",
        "        X_train_fold, X_val_fold = X_train[train_indices], X_train[val_indices]\n",
        "        y_train_fold, y_val_fold = y_train[train_indices], y_train[val_indices]\n",
        "        kmeans = KMeans(n_clusters=n_centers, random_state=42, n_init='auto').fit(X_train_fold)\n",
        "        centers = kmeans.cluster_centers_\n",
        "        phi_train = rbf_kernel(X_train_fold, centers, gamma_val)\n",
        "        phi_val = rbf_kernel(X_val_fold, centers, gamma_val)\n",
        "        model = Ridge(alpha=ridge_alpha).fit(phi_train, y_train_fold)\n",
        "        error = mean_squared_error(y_val_fold, model.predict(phi_val))\n",
        "        fold_errors.append(error)\n",
        "    mean_errors[gamma_val] = np.mean(fold_errors)\n",
        "\n",
        "best_gamma_manual = min(mean_errors, key=mean_errors.get)\n",
        "print(f\"\\nMejor gamma encontrado manualmente: {best_gamma_manual}\")\n",
        "\n",
        "# --- NOVEDAD: Re-entrenamiento y Evaluación Final para el Modelo Manual ---\n",
        "print(\"\\n--- Evaluación Final del Mejor Modelo (Manual) ---\")\n",
        "# 1. Aprender los centros usando TODOS los datos de entrenamiento\n",
        "kmeans_final = KMeans(n_clusters=n_centers, random_state=42, n_init='auto').fit(X_train)\n",
        "final_centers = kmeans_final.cluster_centers_\n",
        "\n",
        "# 2. Transformar el conjunto de entrenamiento completo y el de prueba\n",
        "phi_train_final = rbf_kernel(X_train, final_centers, best_gamma_manual)\n",
        "phi_test_final = rbf_kernel(X_test, final_centers, best_gamma_manual)\n",
        "\n",
        "# 3. Entrenar el modelo final con TODOS los datos de entrenamiento\n",
        "final_model_manual = Ridge(alpha=ridge_alpha)\n",
        "final_model_manual.fit(phi_train_final, y_train)\n",
        "\n",
        "# 4. Reportar el error en el conjunto de prueba\n",
        "final_predictions = final_model_manual.predict(phi_test_final)\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "print(f\"Error Cuadrático Medio en el conjunto de prueba final (Manual): {final_mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CxIrUPcYup4",
        "outputId": "31e08dc0-5557-4c9e-c63f-fb48843bf285"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iniciando K-Fold Manual ---\n",
            "\n",
            "Mejor gamma encontrado manualmente: 50\n",
            "\n",
            "--- Evaluación Final del Mejor Modelo (Manual) ---\n",
            "Error Cuadrático Medio en el conjunto de prueba final (Manual): 15.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Versión 2: GridSearchCV de Scikit-learn ---\n",
        "\n",
        "print(\"\\n--- Iniciando GridSearchCV de Scikit-learn ---\")\n",
        "\n",
        "# --- NOVEDAD: RBFTransformer modificado para usar los datos como centros ---\n",
        "class RBFDataAsCentersTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, gamma_val=1.0):\n",
        "        self.gamma_val = gamma_val\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # En lugar de K-Means, los centros son los propios datos de entrenamiento\n",
        "        self.centers_ = X\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return rbf_kernel(X, self.centers_, self.gamma_val)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('rbf_transformer', RBFDataAsCentersTransformer()),\n",
        "    ('ridge', Ridge())\n",
        "])\n",
        "\n",
        "# --- NOVEDAD: El grid ya no incluye n_centers ---\n",
        "param_grid = {\n",
        "    'rbf_transformer__gamma_val': [10, 50, 100, 200],\n",
        "    'ridge__alpha': [0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nMejores hiperparámetros (GridSearchCV): {grid_search.best_params_}\")\n",
        "print(f\"Mejor score (MSE negativo) en CV: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# --- Evaluación Final del Mejor Modelo (GridSearchCV) ---\n",
        "print(\"\\n--- Evaluación Final en el Conjunto de Prueba (GridSearchCV) ---\")\n",
        "best_model_gs = grid_search.best_estimator_\n",
        "final_predictions_gs = best_model_gs.predict(X_test)\n",
        "final_mse_gs = mean_squared_error(y_test, final_predictions_gs)\n",
        "\n",
        "print(f\"Error Cuadrático Medio en el conjunto de prueba final (GridSearchCV): {final_mse_gs:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiRIgcryZWJ1",
        "outputId": "6bc80068-e400-468e-9543-b48324258424"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iniciando GridSearchCV de Scikit-learn ---\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "Mejores hiperparámetros (GridSearchCV): {'rbf_transformer__gamma_val': 10, 'ridge__alpha': 0.1}\n",
            "Mejor score (MSE negativo) en CV: -12.65\n",
            "\n",
            "--- Evaluación Final en el Conjunto de Prueba (GridSearchCV) ---\n",
            "Error Cuadrático Medio en el conjunto de prueba final (GridSearchCV): 11.91\n"
          ]
        }
      ]
    }
  ]
}