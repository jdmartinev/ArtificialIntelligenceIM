{"cells":[{"cell_type":"markdown","metadata":{"id":"k_d5LbVSeeoj"},"source":["# **Lecture 4d - Normalización de Funciones e Inicialización de Pesos**\n","\n","- Feature/input normalization\n","- Batch normalization\n","- BatchNorm en PyTorch\n","- ¿Por qué funciona BatchNorm?\n","- Inicialización de pesos: ¿por qué es importante?\n","- Inicialización de Xavier y He\n","- Esquemas de inicialización de pesos en PyTorch\n"]},{"cell_type":"markdown","metadata":{"id":"7s6syj6tuGFL"},"source":["# **Input normalization para mejorar gradiente descendente**\n","\n","- **Feature/input normalization**\n","- Batch normalization\n","- BatchNorm en PyTorch\n","- ¿Por qué funciona BatchNorm?\n","- Inicialización de pesos: ¿por qué es importante?\n","- Inicialización de Xavier y He\n","- Esquemas de inicialización de pesos en PyTorch"]},{"cell_type":"markdown","metadata":{"id":"ExcBfrOwuZxz"},"source":["# **Resumen: Por qué se normalizan las entradas para gradiente descendente?**\n","\n","![](Figs/NOR1.PNG)\n","\n","- Normalizar las entradas a la red solo afecta a la primera capa oculta\n","\n","- ¿Qué pasa con las otras capas ocultas?"]},{"cell_type":"markdown","metadata":{"id":"LKCLMxvwvWNG"},"source":["# **Input normalization para mejorar gradiente descendente**\n","\n","- Feature/input normalization\n","- **Batch normalization**\n","- BatchNorm en PyTorch\n","- ¿Por qué funciona BatchNorm?\n","- Inicialización de pesos: ¿por qué es importante?\n","- Inicialización de Xavier y He\n","- Esquemas de inicialización de pesos en PyTorch"]},{"cell_type":"markdown","metadata":{"id":"VjZCX1V9va4z"},"source":["# **Bacth normalization (\"BatchNorm\")**\n","\n","Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning (pp. 448-456).\n","\n","http://proceedings.mlr.press/v37/ioffe15.html"]},{"cell_type":"markdown","metadata":{"id":"MKmHkR9CvwCo"},"source":["# **Batch normalization (\"BatchNorm\")**\n","\n","- Normaliza las entradas de las capas ocultas\n","- Ayuda con problemas de gradientes que explotan / desvanecen\n","- Puede aumentar la estabilidad del entrenamiento y la convergencia.\n","- Puede entenderse como capas adicionales (normalización con parámetros adicionales)\n"]},{"cell_type":"markdown","metadata":{"id":"Yf90mIL0wqWX"},"source":["Suponga, que se tiene una entrada de red $z_{1}^{(2)}$ asociada con una activación en la segunda capa oculta\n","\n","![](Figs/NOR2.PNG)"]},{"cell_type":"markdown","metadata":{"id":"PNKYkC7kxx1j"},"source":["Ahora, considere todos los ejemplos en un minibatch tal que la entrada de red de un ejemplo de entrenamiento dado en la capa 2 se escriba como:\n","##$z_{1}^{(2)[i]}$ donde $i \\in \\left \\{1, ..., n \\right \\}$\n","\n","![](Figs/NOR3.PNG)\n","\n","En las siguientes diapositivas, se omitirá el índice de capa, ya que puede distraer.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BVxLqVAa0m9t"},"source":["# **Paso 1 de BatchNorm: Normalizar las Entradas Netas**\n","\n","![](Figs/NOR4.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"K4OT1PhY1cx5"},"source":["# **Paso 1 de BatchNorm: Normalizar las Entradas Netas**\n","\n","![](Figs/NOR4.PNG)\n","\n","En la práctica:\n","\n","#**${z}'^{[i]}_{j} = \\frac{z_{j}^{[i]} - \\mu_{j}}{\\sqrt{\\sigma _{j}^{2} + \\epsilon }}$**\n","\n","Para estabilidad numérica, donde $\\epsilon$ es un número pequeño ~$1e^{-5}$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"is8dmsE73H9O"},"source":["# **Paso 2 de BatchNorm: Escala de Preactivación**\n","\n","![](Figs/NOR5.PNG)"]},{"cell_type":"markdown","metadata":{"id":"QMj0k850IHJf"},"source":["# **Paso 2 de BatchNorm: Escala de Preactivación**\n","\n","\n","![](Figs/NOR6.PNG)\n","\n","**Técnicamente, una capa de BatchNorm podría aprender a realizar una \"estandarización\" con media cero y varianza unitaria.**\n"]},{"cell_type":"markdown","metadata":{"id":"O897IT3vIYxB"},"source":["# **Resumen de los pasos 1 y 2 de BatchNorm**\n","\n","![](Figs/NOR7.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"jdQAxsW4Ivd1"},"source":["# **BatchNorm: Aspectos Adicionales a Considerar**\n","\n","![](Figs/NOR8.PNG)\n","\n","Además, tenga en cuenta que los parámetros de batchnorm son vectores con el mismo número de elementos que el vector de sesgo\n"]},{"cell_type":"markdown","metadata":{"id":"Ob-JXfZDKIrw"},"source":["# **Cómo usar BatchNorm en la práctica y durante la inferencia**\n","\n","- Feature/input normalization\n","- Batch normalization\n","- **BatchNorm en PyTorch**\n","- ¿Por qué funciona BatchNorm?\n","- Inicialización de pesos: ¿por qué es importante?\n","- Inicialización de Xavier y He\n","- Esquemas de inicialización de pesos en PyTorch"]},{"cell_type":"markdown","metadata":{"id":"vLwMh3TwjDnc"},"source":["![](Figs/NOR9.PNG)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q337W4RbjWBl"},"source":["![](Figs/NOR10.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"2MLzyoHsjk_7"},"source":["![](Figs/NOR11.PNG)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BrErckG8j8bS"},"source":["# **BatchNorm durante la predicción (\"Inferencia\")**\n","\n","- Utilice un promedio ponderado exponencialmente (promedio móvil) de media y varianza\n","\n","running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n","\n","(donde **momentum** es tipicalmente ~0.1; y lo mismo para la varianza)\n","\n","- Alternativamente, también puede usar la media y la varianza del conjunto de entrenamiento global\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gXkzR56roubi"},"source":["# **BatchNorm: algunas teorías y consejos prácticos**\n","\n","- Feature/input normalization\n","- Batch normalization\n","- BatchNorm en PyTorch\n","- **¿Por qué funciona BatchNorm?**\n","- Inicialización de pesos: ¿por qué es importante?\n","- Inicialización de Xavier y He\n","- Esquemas de inicialización de pesos en PyTorch"]},{"cell_type":"markdown","metadata":{"id":"6L9PZiylo5WT"},"source":["# **BatchNorm and internal covariate shift**\n","\n","Ioffe, S., & Szegedy, C. (2015, June). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning (pp. 448-456).\n","\n","http://proceedings.mlr.press/v37/ioffe15.html\n","\n","Internal Covariate Shift es una jerga para decir que la distribución de entrada de la capa cambia (\"cambio de características\" en las capas ocultas)\n","\n","Pero no hay garantía ni evidencia sólida de que BatchNorm ayude con el cambio de covariables\n","\n","Tal vez BatchNorm solo proporcione parámetros adicionales que ayudarán a las capas a aprender un poco más de forma independiente"]},{"cell_type":"markdown","metadata":{"id":"LcOCieL2pxQj"},"source":["![](Figs/NOR12.PNG)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m7osOlHWqHCO"},"source":["# **BatchNorm permite una convergencia más rápida al permitir tasas de aprendizaje más altas**\n","\n","![](Figs/NOR13.PNG)\n","\n","Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization?. In Advances in Neural Information Processing Systems (pp. 2488-2498).\n","\n","https://arxiv.org/abs/1805.11604\n"]},{"cell_type":"markdown","metadata":{"id":"ZmeJsbC_rzKl"},"source":["# **El buen rendimiento de BatchNorm parece no estar relacionado con la prevención de cambios de covariables**\n","\n","![](Figs/NOR14.PNG)\n","\n","Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization?. In Advances in Neural Information Processing Systems (pp. 2488-2498)."]},{"cell_type":"markdown","metadata":{"id":"k5OgrNrLqLMk"},"source":["# **¿Cómo funciona BatchNorm?**\n","\n","![](https://drive.google.com/uc?id=1RCVVk4YcTmkaTqtVramgHqUgXG6cF5Uv)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"psgdP48Wgxq4"},"source":["# **¿Por qué ayuda BatchNorm?**\n","\n","## **2015**\n","## Reduce el cambio de covariables.\n","Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.\n","\n","## **2018**\n","## Las redes con BatchNorm se entrenan bien con o sin ICS. La hipótesis es que BatchNorm suaviza el panorama de la optimización.\n","Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization? In Advances in Neural Information Processing Systems (pp. 2483-2493).\n","\n","## **2018**\n","## \"La normalización por lotes reduce implícitamente la dependencia en una sola dirección\" (en este caso, \"dependencia en una sola dirección\" significa que una entrada influye solo en una unidad única o una combinación lineal de unidades individuales)\n","Morcos, A. S., Barrett, D. G., Rabinowitz, N. C., & Botvinick, M. (2018). On the importance of single directions for generalization. arXiv preprint arXiv:1803.06959."]},{"cell_type":"markdown","metadata":{"id":"tbpZk0kRxxru"},"source":["# **¿Cómo funciona BatchNorm?**\n","# **¿Por qué ayuda BatchNorm?**\n","\n","## **2018**\n","## BatchNorm actúa como un regularizador implícito y mejora la precisión de la generalización\n","Luo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in batch normalization. arXiv preprint arXiv:1809.00846.\n","\n","## **2019**\n","## BatchNorm provoca gradientes explosivos, lo que requiere un ajuste cuidadoso al entrenar redes neuronales profundas sin omitir conexiones (más sobre omitir conexiones pronto)\n","Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., & Schoenholz, S. S. (2019). A mean field theory of batch normalization. arXiv preprint arXiv:1902.08129."]},{"cell_type":"markdown","metadata":{"id":"BWb-vu2Nye6F"},"source":["# **Variantes de BatchNorm**\n","\n","## **Preactivación**\n","Versión \"original\" como se discutió en diapositivas anteriores\n","\n","![](Figs/NOR15.PNG)\n","\n","\n","\n","## **Post-activación**\n","Puede tener más sentido, pero menos común\n","\n","![](Figs/NOR16.PNG)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xLPwPy-Oz1d0"},"source":["# **Algunos puntos de referencias**\n","\n","https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md#bn----before-or-after-relu\n","\n","![](Figs/NOR17.PNG)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QMoVQIAe0SHR"},"source":["# **Consideración Práctica**\n","\n","## BatchNorm se vuelve más estable con tamaños de minibatch más grandes\n","\n","![](Figs/NOR18.PNG)\n","\n","Wu, Y., & He, K. (2018). Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 3-19)."]},{"cell_type":"markdown","metadata":{"id":"UnU7NeN80rdr"},"source":["# **Otras Lecturas**\n","\n","**Conditional batch norm**\n","De Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O., & Courville, A. (2017). Modulating early visual processing by language. arXiv preprint arXiv:1707.00683.\n","\n","https://github.com/pytorch/pytorch/issues/8985\n","\n","https://arxiv.org/abs/1707.00683\n","\n","Shallue, C. J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., & Dahl, G. E. (2018). **Measuring the effects of data parallelism on neural network training.** arXiv preprint arXiv:1811.03600. -> \"We find no evidence that larger batch sizes degrade out-of-sample performance\"\n","\n","https://arxiv.org/abs/1811.03600\n","\n","Cai, Z., Ravichandran, A., Maji, S., Fowlkes, C., Tu, Z., & Soatto, S. (2021). **Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning**. arXiv preprint arXiv:2101.08482. -> \"We present a plug-in replacement for batch normalization (BN) called exponential moving average normalization (EMAN), which improves the performance of existing student-teacher based self- and semisupervised learning techniques. ...\"\n","\n","https://arxiv.org/abs/2101.08482\n","\n","Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021). **High-Performance Large-Scale Image Recognition\n","Without Normalization.** arXiv preprint arXiv:2102.06171. “Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets.”\n","\n","https://arxiv.org/abs/2102.06171"]},{"cell_type":"markdown","metadata":{"id":"8lnAdhbN1lCr"},"source":["# **¿Por qué tamaños de minibatch como potencias de 2?**\n","\n","- Relacionado con SIMD - Datos múltiples de instrucción única - paradigma utilizado por CPUs/GPUs\n","- Proviene de mapear los cálculos (por ejemplo, productos punto) a los núcleos de procesamiento físico en la GPU, donde el número de núcleos de procesamiento suele ser una potencia de 2\n","- Por ejemplo, si tenemos 32 columnas en una matriz, podemos asignar 2 productos punto a cada núcleo de procesamiento si tenemos 16 núcleos de procesamiento (las GPU generalmente tienen muchos, muchos más núcleos de procesamiento)\n","\n","![](Figs/NOR19.PNG)\n","\n","fuente: https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/SIMD2.svg/440pxSIMD2.svg.png"]},{"cell_type":"markdown","metadata":{"id":"z45dISax3CQn"},"source":["# **Además de la normalización de entrada, la inicialización del peso también es importante**\n","\n","# **BatchNorm: algunas teorías y consejos prácticos**\n","\n","- Feature/input normalization\n","- Batch normalization\n","- BatchNorm en PyTorch\n","- ¿Por qué funciona BatchNorm?\n","- **Inicialización de pesos: ¿por qué es importante?**\n","- Inicialización de Xavier y He\n","- Esquemas de inicialización de pesos en PyTorchh"]},{"cell_type":"markdown","metadata":{"id":"sfH_TtpC3Oc6"},"source":["# **Inicialización de pesos**\n","\n","- Anteriormente se discutió que queremos inicializar los pesos en números pequeños y aleatorios para romper la simetría\n","- Además, se quiere que los pesos sean relativamente pequeños, ¿por qué?\n","\n","![](Figs/NOR20.PNG)"]},{"cell_type":"markdown","metadata":{"id":"VBbQDcmL5q9D"},"source":["# **Nota: Problemas de gradientes que desvanecen/explotan**\n","\n","![](Figs/NOR21.PNG)"]},{"cell_type":"markdown","metadata":{"id":"YIRHpKfL59mq"},"source":["# **Nota: Problemas de gradientes que desvanecen/explotan**\n","\n","![](Figs/NOR22.PNG)"]},{"cell_type":"markdown","metadata":{"id":"FEZyK89c6Muk"},"source":["# **Nota: Problemas de gradientes que desvanecen/explotan**\n","\n","Supongamos que tenemos el gradiente más grande:\n","\n","## $\\frac{\\mathrm{d} }{\\mathrm{d} z}\\sigma (0.0) = \\sigma (0.0)(1-\\sigma(0.0)) = 0.25$\n","\n","Incluso entonces, por ejemplo, para 10 capas, ¡degradamos sustancialmente los otros gradientes!\n","\n","## $0.25^{10} \\approx 10^{-6}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A_TTlQxR6_iH"},"source":["# **Inicialización de Peso**\n","\n","- Tradicionalmente, podemos inicializar pesos muestreando una distribución uniforme aleatoria en el rango [0, 1], o mejor, [-0.5, 0.5]\n","- O, podríamos tomar muestras de una distribución gaussiana con media 0 y pequeña varianza (por ejemplo, 0,1 o 0,01)\n"]},{"cell_type":"markdown","metadata":{"id":"bTMB_xtx7fX7"},"source":["# **BatchNorm: algunas teorías y consejos prácticos**\n","\n","- Normalización de entrada\n","- Normalización por lotes\n","- BatchNorm en PyTorch\n","- ¿Por qué funciona BatchNorm?\n","- Inicialización de pesos: ¿por qué nos importa?\n","- **Inicialización de Xavier y He**\n","- Esquemas de inicialización de peso en PyTorch"]},{"cell_type":"markdown","metadata":{"id":"SXhb1OPn7kVe"},"source":["# **Inicialización de Peso - Inicialización de Xavier**\n","\n","Xavier Glorot and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","- TanH es un poco más robusto con respecto a los gradientes explosivos (en comparación con el sigmoide logístico)\n","- Todavía tiene el problema de la saturación (gradientes cercanos a cero si las entradas son valores muy grandes, positivos o negativos)\n","- La inicialización de Xavier es una pequeña mejora para la inicialización de pesos para tanH"]},{"cell_type":"markdown","metadata":{"id":"ZHwZenl78mNA"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Xavier Glorot and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","**Paso 1:** Inicializar pesos basándonos en distribuciones gaussianas o uniformes\n","\n","**Paso 2:** Escale los pesos proporcionalmente al número de entradas a la capa\n","\n","(Para la primera capa oculta, esa es la cantidad de características en el conjunto de datos; para la segunda capa oculta, esa es la cantidad de unidades en la primera capa oculta, etc.)"]},{"cell_type":"markdown","metadata":{"id":"eSnn3yGe9riW"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Xavier Glorot and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","## **Método:**\n","\n","Escale los pesos proporcionalmente al número de entradas a la capa\n","\n","En particular, escale de la siguiente manera:\n","\n","$W^{(l)} := W^{(l)}\\cdot \\sqrt{\\frac{1}{m^{(l-1)}}}$\n","\n","donde $m$ es el número de unidades de entrada a la siguente capa\n","\n","Por ejemplo: $W_{i,j}^{(l)} \\sim N(\\mu = 0, \\sigma ^{2} = 0.01)$\n","\n","(o distribución uniforme en un intervalo fijo, como en el artículo original)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NLJTkcfX-4ds"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Xavier Glorot and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","**Nota: Si no inicializó las unidades de sesgo a todos los ceros, inclúyalas también en la escala.**\n","\n","$W^{(l)} := W^{(l)}\\cdot \\sqrt{\\frac{1}{m^{(l-1)}}}$\n","\n","donde $m$ es el número de unidades de entrada a la siguente capa\n","\n","Por ejemplo: $W_{i,j}^{(l)} \\sim N(\\mu = 0, \\sigma ^{2} = 0.01)$\n","\n","(o distribución uniforme en un intervalo fijo, como en el artículo original)"]},{"cell_type":"markdown","metadata":{"id":"0WQlUN9g_Jjf"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","**Justificación detrás de esta escala:\n","La varianza de la muestra (entre los puntos de datos, no la varianza de la media) aumenta linealmente a medida que aumenta el tamaño de la muestra (la varianza de la suma de las variables independientes es la suma de las varianzas); raíz cuadrada de la desviación estándar**\n","\n","$W^{(l)} := W^{(l)}\\cdot \\sqrt{\\frac{1}{m^{(l-1)}}}$\n","\n","donde $m$ es el número de unidades de entrada a la siguente capa\n","\n","Por ejemplo: $W_{i,j}^{(l)} \\sim N(\\mu = 0, \\sigma ^{2} = 0.01)$\n","\n","(o distribución uniforme en un intervalo fijo, como en el artículo original)"]},{"cell_type":"markdown","metadata":{"id":"VuxmFDZYATSZ"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","**Justificación detrás de esta escala:\n","La varianza de la muestra (entre los puntos de datos, no la varianza de la media) aumenta linealmente a medida que aumenta el tamaño de la muestra (la varianza de la suma de las variables independientes es la suma de las varianzas); raíz cuadrada de la desviación estándar**\n","\n","![](Figs/NOR23.PNG)"]},{"cell_type":"markdown","metadata":{"id":"Uu4SS37EAnQd"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","![](Figs/NOR24.PNG)"]},{"cell_type":"markdown","metadata":{"id":"GFDYOlfIBDhq"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","![](Figs/NOR24.PNG)\n","\n","**Sin embargo, en la práctica, algunas personas también usan \"entrada de ventilador\" + \"salida de ventilador\" en el denominador, y funciona bien.**"]},{"cell_type":"markdown","metadata":{"id":"F8HxV8ppBZdD"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","![](Figs/NOR25.PNG)"]},{"cell_type":"markdown","metadata":{"id":"_rsvxLPOB-Ne"},"source":["# **Inicialización de Pesos - Inicialización de Xavier**\n","\n","Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n","\n","![](Figs/NOR26.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"AAuHTwtECNbb"},"source":["# **Inicialización de Pesos - Inicialización de He**\n","\n","Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.\n","\n","- Suponiendo activaciones con media 0, que es razonable, inicialización de Xavier asume una derivada de 1 para la función de activación (que es razonable para tanH)\n","- Para ReLU, esto es diferente, ya que las activaciones ya no están centradas en cero\n","- La inicialización de He tiene esto en cuenta\n","- El resultado es que agregamos un factor de escala de $2$$\n","\n","#$W^{(l)} := W^{(l)}\\cdot \\sqrt{\\frac{2}{m^{(l-1)}}}$\n"]},{"cell_type":"markdown","metadata":{"id":"OGG3IRHVC4R8"},"source":["# **¿Cómo maneja PyTorch la inicialización del pesos y cómo la anulamos?**\n","\n","# **BatchNorm: algunas teorías y consejos prácticos**\n","\n","- Normalización de entrada\n","- Normalización por lotes\n","- BatchNorm en PyTorch\n","- ¿Por qué funciona BatchNorm?\n","- Inicialización de pesos: ¿por qué nos importa?\n","- Inicialización de Xavier y He\n","- **Esquemas de inicialización de peso en PyTorch**"]},{"cell_type":"markdown","metadata":{"id":"NEWX-87fDI6Q"},"source":["# **Pesos predeterminados de PyTorch**\n","\n","PyTorch (ahora) usa el esquema Kaiming He por defecto\n","\n","![](Figs/NOR27.PNG)\n","\n","https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L86"]},{"cell_type":"markdown","metadata":{"id":"eR2QU7JwDeYi"},"source":["![](Figs/NOR28.PNG)"]},{"cell_type":"markdown","metadata":{"id":"FWhkIhdyDotl"},"source":["![](Figs/NOR29.PNG)\n","\n","![](Figs/NOR30.PNG)"]},{"cell_type":"markdown","metadata":{"id":"fEmmDPFQED2c"},"source":["# **Tenga en cuenta que si se utiliza BatchNorm, la elección del peso de la característica inicial es menos importante de todos modos**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3anOcErmEQsH"},"source":["![](Figs/NOR31.PNG)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6580,"status":"ok","timestamp":1633095276734,"user":{"displayName":"Juan David Martinez Vargas","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10271309777444916336"},"user_tz":300},"id":"i2Be3HsoClrH","outputId":"c8ca1358-d01f-4566-c103-d1c6516635fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["File ‘colab_pdf.py’ already there; not retrieving.\n","\n","\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n","\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n","[NbConvertApp] WARNING | pattern u'/content/drive/MyDrive/Colab Notebooks//content/drive/MyDrive/Classroom/Aprendizaje profundo 2021 II/Lecture06/Info/Lecture06_Profe.ipynb' matched no files\n","This application is used to convert notebook files (*.ipynb) to various other\n","formats.\n","\n","WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n","\n","Options\n","-------\n","\n","Arguments that take values are actually convenience aliases to full\n","Configurables, whose aliases are listed on the help line. For more information\n","on full configurables, see '--help-all'.\n","\n","--execute\n","    Execute the notebook prior to export.\n","--allow-errors\n","    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n","--no-input\n","    Exclude input cells and output prompts from converted document. \n","    This mode is ideal for generating code-free reports.\n","--stdout\n","    Write notebook output to stdout instead of files.\n","--stdin\n","    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n","--inplace\n","    Run nbconvert in place, overwriting the existing notebook (only \n","    relevant when converting to notebook format)\n","-y\n","    Answer yes to any questions instead of prompting.\n","--clear-output\n","    Clear output of current file and save in place, \n","    overwriting the existing notebook.\n","--debug\n","    set log level to logging.DEBUG (maximize logging output)\n","--no-prompt\n","    Exclude input and output prompts from converted document.\n","--generate-config\n","    generate default config file\n","--nbformat=<Enum> (NotebookExporter.nbformat_version)\n","    Default: 4\n","    Choices: [1, 2, 3, 4]\n","    The nbformat version to write. Use this to downgrade notebooks.\n","--output-dir=<Unicode> (FilesWriter.build_directory)\n","    Default: ''\n","    Directory to write output(s) to. Defaults to output to the directory of each\n","    notebook. To recover previous default behaviour (outputting to the current\n","    working directory) use . as the flag value.\n","--writer=<DottedObjectName> (NbConvertApp.writer_class)\n","    Default: 'FilesWriter'\n","    Writer class used to write the  results of the conversion\n","--log-level=<Enum> (Application.log_level)\n","    Default: 30\n","    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n","    Set the log level by value or name.\n","--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n","    Default: u''\n","    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n","    but can be any url pointing to a copy  of reveal.js.\n","    For speaker notes to work, this must be a relative path to a local  copy of\n","    reveal.js: e.g., \"reveal.js\".\n","    If a relative path is given, it must be a subdirectory of the current\n","    directory (from which the server is run).\n","    See the usage documentation\n","    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n","    slideshow) for more details.\n","--to=<Unicode> (NbConvertApp.export_format)\n","    Default: 'html'\n","    The export format to be used, either one of the built-in formats\n","    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n","    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n","    the import path for an `Exporter` class\n","--template=<Unicode> (TemplateExporter.template_file)\n","    Default: u''\n","    Name of the template file to use\n","--output=<Unicode> (NbConvertApp.output_base)\n","    Default: ''\n","    overwrite base name use for output files. can only be used when converting\n","    one notebook at a time.\n","--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n","    Default: u''\n","    PostProcessor class used to write the results of the conversion\n","--config=<Unicode> (JupyterApp.config_file)\n","    Default: u''\n","    Full path of a config file.\n","\n","To see all available configurables, use `--help-all`\n","\n","Examples\n","--------\n","\n","    The simplest way to use nbconvert is\n","    \n","    > jupyter nbconvert mynotebook.ipynb\n","    \n","    which will convert mynotebook.ipynb to the default format (probably HTML).\n","    \n","    You can specify the export format with `--to`.\n","    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n","    \n","    > jupyter nbconvert --to latex mynotebook.ipynb\n","    \n","    Both HTML and LaTeX support multiple output templates. LaTeX includes\n","    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n","    can specify the flavor of the format used.\n","    \n","    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n","    \n","    You can also pipe the output to stdout, rather than a file\n","    \n","    > jupyter nbconvert mynotebook.ipynb --stdout\n","    \n","    PDF is generated via latex\n","    \n","    > jupyter nbconvert mynotebook.ipynb --to pdf\n","    \n","    You can get (and serve) a Reveal.js-powered slideshow\n","    \n","    > jupyter nbconvert myslides.ipynb --to slides --post serve\n","    \n","    Multiple notebooks can be given at the command line in a couple of \n","    different ways:\n","    \n","    > jupyter nbconvert notebook*.ipynb\n","    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n","    \n","    or you can specify the notebooks list in a config file, containing::\n","    \n","        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n","    \n","    > jupyter nbconvert --config mycfg.py\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'File Download Unsuccessful. Saved in Google Drive'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n","from colab_pdf import colab_pdf\n","colab_pdf('/content/drive/MyDrive/Classroom/Aprendizaje profundo 2021 II/Lecture06/Info/Lecture06_Profe.ipynb')"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
